\chapter{Conclusion and Future Work}

\section{Conclusion}

We have presented a benchmark framework named StreamBench, which aims to provide tools for apples-to-apples comparison of different stream processing systems. One contribution of the benchmark is the definition of three core workloads and corresponding data generators, which begin to fill out the space of evaluating performance of these systems performing different operators. Another contribution of the benchmark is its extensibility property. New workloads can be easily created, including generalized workloads to examine system fundamentals, as well as more domain-specific workloads to model particular applications. As an open-source project, developers also could extend StreamBench to evaluate other stream processing systems. Currently, there is not a standard benchmark framework for stream processing systems yet. The extensibility of StreamBench allows the stream processing community extend and develop it to be a standard benchmark framework. 

We have used this tool to benchmark the performance of three stream processing systems, and observed that the throughput of Flink and Spark Streaming is much better than Storm, but Storm achieves a much lower median latency in most workloads. The AdvClick workloads also indicates that Storm and Flink provide low level APIs, and it is flexible for users to implement new operators with these APIs. These results highlight the importance of a standard framework for examining system performance so that developers can select the most appropriate system in practic.

\section{Future Work}

In addition to performance comparisons, other aspects of stream processing systems are also very important factors when selecting a system to deal with stream data in practice, such as scalability, elastic speedup, and availability. For example, a stream processing application is running in a cluster, it is possible that the speed of input stream keeps increasing and becomes larger than cluster's throughput. In this case, scalability is very important which indicates how much the processing ability of a cluster could increase by adding more compute nodes. In our future works, we will implement more workload to examine these aspects of stream processing systems.

Beside design new workloads, we also could extend StreamBench to benchmark other stream processing systems. In the thesis, we only selected three stream processing systems to run the benchmarks. There are many other stream processing systems that are widely used, such as Amazon Kinesis and Apache Samza. Extending StreamBench to evaluate these systems is listed in our future work.

There are some issues in StreamBench that could be improved or fixed in the future. First, the speed of data generation can't be controlled precisely. It fluctuates around some point. The throughputs mentioned in this thesis are all approximate value. Another issue we noticed is the amount of data in Kafka affects the performance of Offline WordCount, especially for Storm. How the amount of data in Kafka and other features of Kafka cluster affect the performance of stream processing is a very interesting research topic. It might help us evaluate the integration between system processing systems and distributed message systems. As mentioned in \cref{subsec:online_wordcount}, currently, Flink doesn't support pre-aggregation and parallel window could only be applied on keyed stream. It is possible to implement Windowed WordCount with Flink's low level API and could be done in our future work. 

%Storm delay
\clearpage


